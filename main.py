import os
import json
import getpass
import math
import random
import time
import sys
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any
import concurrent.futures
from tqdm import tqdm
from openai import OpenAI
import pandas as pd
import numpy as np

try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    SentenceTransformer = None
try:
    import MeCab
    import ipadic
except ImportError:
    MeCab = None
    ipadic = None

from sklearn.metrics.pairwise import cosine_similarity

# ==========================================
# 0. åˆæœŸè¨­å®šã¨å®šæ•°
# ==========================================
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter OPENAI_API_KEY: ")
client = OpenAI(
    timeout=900.0,  
    max_retries=3    
)

STOP_WORDS = [
    'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã¨', 'ã¦', 'ã§', 'ã§ã™', 'ã¾ã™', 
    'ã—ãŸ', 'ã„ã‚‹', 'ã‚ã‚‹', 'ã‚ˆã†ãª', 'ã‚ˆã†ã«', 'è¦‹ãˆ', 'è¦‹ãˆã‚‹', 
    'å½¢', 'æ§˜å­', 'å…¨ä½“', 'éƒ¨åˆ†', 'å›³å½¢', 'ã‚¿ãƒ³ã‚°ãƒ©ãƒ ', 'æ€ã†', 'æ„Ÿã˜',
    'ç§', 'ã‚ãªãŸ', 'ãã‚Œ', 'ã“ã‚Œ', 'ã‚ã‚Œ', 'ã“ã¨', 'ã‚‚ã®'
]

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SAVE_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(SAVE_DIR, exist_ok=True)

# ==========================================
# 1. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°å®šç¾©
# ==========================================
def call_api_with_retry(func, max_retries=5, initial_wait=1.0):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            error_msg = str(e)
            if any(x in error_msg for x in ["Rate limit", "429", "500", "503", "502"]):
                if attempt == max_retries - 1: raise e
                wait_time = (initial_wait * (2 ** attempt)) + random.uniform(0, 1)
                time.sleep(wait_time)
            else:
                raise e

def _sanitize_str(val: Any) -> Optional[str]:
    if val is None: return None
    if isinstance(val, (dict, list)): return str(val) 
    return str(val)

class suppress_output:
    def __init__(self, suppress=True): pass
    def __enter__(self): pass
    def __exit__(self, exc_type, exc_val, exc_tb): pass

# ==========================================
# 2. ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
# ==========================================
class DataManager:
    _instance = None
    _model = None       
    
    def __init__(self, csv_path="data.csv"):
        print(f"ğŸ“¥ Loading DataManager from {csv_path}...")
        self.csv_path = csv_path
        self.df = pd.DataFrame()
        self.tangram_centroids = {}
        self.all_labels = []
        self.context_text = "" 
        
        # SBERTãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        if DataManager._model is None:
            if SentenceTransformer is None:
                print("âš ï¸ sentence-transformers not found.")
                DataManager._model = None
            else:
                try:
                    print("â³ Loading SBERT model...")
                    DataManager._model = SentenceTransformer('sonoisa/sentence-bert-base-ja-mean-tokens', device="cpu",model_kwargs={"low_cpu_mem_usage": False})
                    print("âœ… SBERT Loaded successfully.")
                except Exception as e:
                    print(f"âš ï¸ SBERT Load Failed: {e}")
                    DataManager._model = None
        
        self.model = DataManager._model
        
        # MeCabã®åˆæœŸåŒ–
        self.tagger = None
        if MeCab and ipadic:
            try:
                self.tagger = MeCab.Tagger(ipadic.MECAB_ARGS + " -Ochasen")
                print("âœ… MeCab Tagger initialized.")
            except Exception as e:
                print(f"âš ï¸ MeCab Init Failed: \n{e}")
        else:
            print("âš ï¸ MeCab or ipadic library not found. Using raw text for similarity.")

        self._load_and_process()

    def _remove_stopwords(self, text):
        if not isinstance(text, str): return ""
        for word in STOP_WORDS:
            text = text.replace(word, " ")
        return " ".join(text.split())

    def extract_features_from_text(self, text: str) -> str:
        if not self.tagger or not text:
            return self._remove_stopwords(text)
        
        try:
            node = self.tagger.parseToNode(text)
            keywords = []
            while node:
                features = node.feature.split(",")
                pos = features[0]
                word = node.surface
                if pos in ["åè©", "å½¢å®¹è©", "å‹•è©"]:
                    if word not in STOP_WORDS:
                        keywords.append(word)
                node = node.next
            return " ".join(keywords)
        except Exception:
            return self._remove_stopwords(text)

    def _load_and_process(self):
        if not os.path.exists(self.csv_path):
            print(f"âš ï¸ {self.csv_path} not found. Creating dummy data.")
            dummy_data = {
                'label': ['A1']*5 + ['A2']*5 + ['B1']*5 + ['B2']*5 + ['B3']*5 + ['C1']*5,
                'text': [f'ç‰¹å¾´_{i}' for i in range(30)],
                'exp': ['Holistic']*15 + ['Analytic']*15
            }
            self.df = pd.DataFrame(dummy_data)
        else:
            self.df = pd.read_csv(self.csv_path)

        self.df['processed_text'] = self.df['text'].apply(self._remove_stopwords)
        self.all_labels = sorted(self.df['label'].unique().tolist())

        context_lines = []
        for label in self.all_labels:
            features = self.df[self.df['label'] == label]['text'].tolist()
            feat_str = " / ".join(features)
            context_lines.append(f"ã€ID: {label}ã€‘\nç‰¹å¾´: {feat_str}\n")
        self.context_text = "\n".join(context_lines)

        if self.model:
            print("ğŸ§® Calculating Centroids...")
            for label in self.all_labels:
                texts = self.df[self.df['label'] == label]['processed_text'].tolist()
                if texts:
                    vectors = self.model.encode(texts)
                    centroid = np.mean(vectors, axis=0)
                    self.tangram_centroids[label] = centroid
        else:
            for label in self.all_labels:
                self.tangram_centroids[label] = np.random.rand(768)

    def get_most_distinct_target(self, candidate_labels: List[str]) -> str:
        if not candidate_labels: return None
        if len(candidate_labels) == 1: return candidate_labels[0]
        if self.model is None: return random.choice(candidate_labels)

        target_vectors = np.array([self.tangram_centroids[l] for l in candidate_labels])
        sim_matrix = cosine_similarity(target_vectors)
        avg_similarities = np.mean(sim_matrix, axis=1)
        min_sim_idx = np.argmin(avg_similarities)
        return candidate_labels[min_sim_idx]
    
    def encode_text(self, text: str):
        if self.model and text:
            filtered_text = self.extract_features_from_text(text)
            if not filtered_text.strip(): 
                filtered_text = text
            return self.model.encode([filtered_text])[0]
        return np.zeros(768)

# ==========================================
# 3. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¯ãƒ©ã‚¹å®šç¾©
# ==========================================

#ãƒªãƒ¼ãƒ€
class LeaderAgent:
    def __init__(self, name="Leader", max_char_count=50, csv_path="data.csv"):
        self.name = name
        self.max_char_count = max_char_count
        self.data_manager = DataManager(csv_path)
        self.log_buffer: List[str] = []
        self.current_target_id = None
        self.unnamed_candidates = list(self.data_manager.all_labels)
        self.named_map = {} 
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.expression_counts = {"Holistic": 0, "Analytic": 0, "Mixed": 0}

    def log(self, msg):
        self.log_buffer.append(f"[{self.name}] {msg}")

    def select_next_target(self):
        if not self.unnamed_candidates:
            return None
        best_target = self.data_manager.get_most_distinct_target(self.unnamed_candidates)
        self.current_target_id = best_target
        self.log(f"ğŸ¯ Target Selected: {self.current_target_id}")
        return self.current_target_id

    def generate_utterance(self, full_history: str, is_naming_phase: bool = False) -> Dict:
        if not self.current_target_id:
            return {"utterance": "çµ‚äº†ã§ã™ã€‚", "thought_process": "å®Œäº†", "strategy": "None"}

        target_features = self.data_manager.df[
            self.data_manager.df['label'] == self.current_target_id
        ]['text'].tolist()
        features_snippet = "\n".join(target_features)

        if is_naming_phase:
            task_instruction = f"""
ã€ç¾åœ¨ã®çŠ¶æ³ã€‘
ç›¸æ‰‹ã¨ã®å…±é€šèªè­˜ãŒå½¢æˆã•ã‚Œã¾ã—ãŸã€‚**å‘½åãƒ•ã‚§ãƒ¼ã‚º**ã«ç§»è¡Œã—ã¾ã™ã€‚
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå›³å½¢ã€{self.current_target_id}ã€‘ã«ãµã•ã‚ã—ã„ã€çŸ­ãè¦šãˆã‚„ã™ã„**ã€Œåå‰ã€**ã‚’ä¸€ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚
ãŸã ã—ï¼Œã“ã‚Œã¾ã§ã®å¯¾è©±å±¥æ­´ã‹ã‚‰ã‚ã‹ã‚‹ä»Šã¾ã§ã«å‘½åã—ãŸåå‰ã¨ã¯è¢«ã‚‰ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼
ã¾ãŸã€ç™ºè©±å†…å®¹ã«ã‚¿ãƒ³ã‚°ãƒ©ãƒ IDã¯å«ã‚ãªã„ã§ãã ã•ã„ï¼
ç™ºè©±å†…å®¹ã¯æœ€å¤§ã§ã‚‚ã²ã‚‰ãŒãªã«ã—ãŸæ™‚ã«{self.max_char_count}æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ï¼
åã‚ãªãŒã‚‰å‘½åææ¡ˆãŒã§ããªã„å ´åˆã¯ã€Œã€ã€œã€ã§ã€ã®ã‚ˆã†ã«æœ€ä½é™ã®æ–‡å­—æ•°ã§ï¼Œç™ºè©±å†…å®¹ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
ãŸã ã—ï¼Œç™ºè©±ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹éš›ã¯ï¼Œæ¼¢å­—ã‚„ã‚«ã‚¿ã‚«ãƒŠã«ã™ã¹ãèªå¥ã¯å¿…ãšå¤‰æ›ã—ã¦ã‹ã‚‰å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
éŸ³å£°å¯¾è©±ã¨ã—ã¦è‡ªç„¶ãªç™ºè©±ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
è‡ªç„¶å¯¾è©±ã‚’æƒ³å®šã™ã‚‹ãŸã‚ç®‡æ¡æ›¸ãã‚„æ‹¬å¼§æ›¸ããªã©ï¼Œè‡ªç„¶å¯¾è©±ã§ã¯ä½¿ç”¨ã•ã‚Œãªã„è¡¨ç¾ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼

**æŠ½å‡ºæŒ‡ç¤º:**
è‡ªåˆ†ã®ç”Ÿæˆã—ãŸç™ºè©±(utterance)ã®ä¸­ã«å«ã¾ã‚Œã‚‹ä»¥ä¸‹ã®è¡¨ç¾ã‚’**æŠ½å‡ºã—ã¦ãƒªã‚¹ãƒˆå½¢å¼ã§**JSONã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
- **Analyticãªè¡¨ç¾**: éƒ¨åˆ†çš„ãƒ»å¹¾ä½•å­¦çš„ãªç‰¹å¾´
- **Holisticãªè¡¨ç¾**: å…¨ä½“çš„ãƒ»æŠ½è±¡çš„ãªå°è±¡

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSON)ã€‘
{{
  "thought_process": "...",
  "strategy": "Naming_Proposal",
  "analytic_expressions": [],
  "holistic_expressions": ["ã‚¦ã‚µã‚®"],
  "utterance": "ãã‚Œã§ã¯ã€ã“ã®å›³å½¢ã‚’ã€ã‚¦ã‚µã‚®ã€ã¨å‘¼ã³ã¾ã›ã‚“ã‹ï¼Ÿ",
  "proposed_name": "ã‚¦ã‚µã‚®"
}}
"""
        else:
            remaining_count = len(self.unnamed_candidates)
            remaining_ids_str = ", ".join(self.unnamed_candidates)
            
            task_instruction = f"""
ã€ç›®æ¨™è¨­å®š: {self.max_char_count}æ–‡å­—ä»¥å†…ã®èª¬æ˜ã€‘
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå›³å½¢ã‚’ï¼ŒæŒ‡å®šã—ãŸæ–‡å­—æ•°ã§å¯èƒ½ãªé™ã‚Šè©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ï¼
ã¾ãŸï¼Œç›¸æ‰‹ã®è¦‹ã¦ã„ã‚‹å›³å½¢ã¯è‡ªåˆ†ã®è¦‹ã¦ã„ã‚‹å›³å½¢ã¨å›è»¢è§’åº¦ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ï¼
**æŒ‡ç¤º:**
ç™ºè©±å†…å®¹ã¯**ã²ã‚‰ãŒãªã«ã—ãŸæ™‚ã«{self.max_char_count}æ–‡å­—ã»ã©**ã«ãªã‚‹ã‚ˆã†ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆå›³å½¢ã®èª¬æ˜ã«ã—ã¦ãã ã•ã„ã€‚
èª¤å·®ã¯ãƒ—ãƒ©ã‚¹ãƒã‚¤ãƒŠã‚¹3æ–‡å­—ã¾ã§ã§ã™ï¼
ãŸã ã—ï¼Œç™ºè©±ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹éš›ã¯ï¼Œæ¼¢å­—ã‚„ã‚«ã‚¿ã‚«ãƒŠã«ã™ã¹ãèªå¥ã¯å¿…ãšå¤‰æ›ã—ã¦ã‹ã‚‰å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
éŸ³å£°å¯¾è©±ã¨ã—ã¦è‡ªç„¶ãªç™ºè©±ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
ãŸã ã—ï¼Œè‡ªç„¶å¯¾è©±ã‚’æƒ³å®šã™ã‚‹ãŸã‚ç®‡æ¡æ›¸ãã‚„æ‹¬å¼§æ›¸ããªã©ï¼Œè‡ªç„¶å¯¾è©±ã§ã¯ä½¿ç”¨ã•ã‚Œãªã„è¡¨ç¾ã¯ç™ºè©±å†…å®¹(utterance)ã§ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼
ã¾ãŸã€ç™ºè©±å†…å®¹ã«ã‚¿ãƒ³ã‚°ãƒ©ãƒ IDã¯å«ã‚ãªã„ã§ãã ã•ã„ï¼

ã¾ãŸï¼Œå¯¾è©±å±¥æ­´ã‚’è¦‹ã¦ï¼Œä»Šã®ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®èª¬æ˜ã§éå»ã«ã™ã§ã«ä¼ãˆã¦ã„ãŸç‰¹å¾´ã‚„è¦ç´ ã¯ç™ºè©±ã«ã¯å«ã‚ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼
åŸºæœ¬çš„ã«ã¯ä»Šã®å¯¾è©±ã‹ã‚‰1ã¤å‰ã®å‘½åã®å¯¾è©±ã¾ã§ã«ä¼ãˆã¦ã„ãŸç‰¹å¾´ã‚„è¦ç´ ã¯ã™ã§ã«ä¼ãˆã¦ã„ã‚‹æƒ…å ±ã«ãªã‚Šã¾ã™ï¼

**æŠ½å‡ºæŒ‡ç¤º:**
è‡ªåˆ†ã®ç”Ÿæˆã—ãŸç™ºè©±(utterance)ã®ä¸­ã«å«ã¾ã‚Œã‚‹ä»¥ä¸‹ã®è¡¨ç¾ã‚’**æŠ½å‡ºã—ã¦ãƒªã‚¹ãƒˆå½¢å¼ã§**JSONã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è©²å½“ãªã—ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ `[]` ã«ã—ã¦ãã ã•ã„ã€‚
ç›´å‰ã®ç›¸æ‰‹ï¼ˆFollowerï¼‰ã®ç™ºè©±ã‹ã‚‰ã¯æŠ½å‡ºã—ãªã„ã§ãã ã•ã„ï¼
- **Analyticãªè¡¨ç¾**: éƒ¨åˆ†çš„ãƒ»å¹¾ä½•å­¦çš„ãªç‰¹å¾´ã®ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹ï¼š"ä¸‰è§’å½¢ãŒã‚ã‚‹", "å³å´ãŒå°–ã£ã¦ã„ã‚‹"ï¼‰
- **Holisticãªè¡¨ç¾**: å…¨ä½“çš„ãƒ»æŠ½è±¡çš„ãªå°è±¡ã®ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹ï¼š"èµ°ã£ã¦ã„ã‚‹äººã®ã‚ˆã†ã ", "ä¸å®‰å®šãªæ„Ÿã˜"ï¼‰

**ç¾åœ¨ã®çŠ¶æ³åˆ†æ:**
- æœªå‘½åã®å€™è£œ: [{remaining_ids_str}] (è¨ˆ{remaining_count}å€‹)

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSON)ã€‘
{{
  "thought_process": "...",
  "strategy": "Description",
  "analytic_expressions": ["..."],
  "holistic_expressions": ["..."],
  "utterance": "èµ°ã£ã¦ã„ã‚‹äººã®ã‚ˆã†ãªã‚¿ãƒ³ã‚°ãƒ©ãƒ ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
  "proposed_name": null
}}
"""

        prompt = f"""
ã‚ãªãŸã¯ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã‚²ãƒ¼ãƒ ã®ã€Œå‡ºé¡Œè€…ï¼ˆLeaderï¼‰ã€ã§ã™ã€‚
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå›³å½¢ã€{self.current_target_id}ã€‘ã«ã¤ã„ã¦è©±ã—ã¦ã„ã¾ã™ã€‚

ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã€‘
{features_snippet}

ã€å…¨ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®ç‰¹å¾´ãƒªã‚¹ãƒˆï¼ˆå‚è€ƒç”¨ï¼‰ã€‘
{self.data_manager.context_text}

{task_instruction}

ã€ã“ã“ã¾ã§ã®å…¨å¯¾è©±å±¥æ­´ã€‘
{full_history}
"""
        res = call_api_with_retry(lambda: client.chat.completions.create(
            model="gpt-5", 
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            reasoning_effort="low"
        ))
        
        self.total_prompt_tokens += res.usage.prompt_tokens
        self.total_completion_tokens += res.usage.completion_tokens
        
        try:
            data = json.loads(res.choices[0].message.content)
        except:
            data = {"utterance": "Error", "thought_process": "Error", "proposed_name": None}

        
        analytic_list = data.get("analytic_expressions", [])
        holistic_list = data.get("holistic_expressions", [])
        
        if not isinstance(analytic_list, list): analytic_list = []
        if not isinstance(holistic_list, list): holistic_list = []

        has_analytic = len(analytic_list) > 0
        has_holistic = len(holistic_list) > 0
        final_type = "None"

        
        self.expression_counts["Analytic"] += len(analytic_list)
        self.expression_counts["Holistic"] += len(holistic_list)

        if has_analytic and has_holistic:
            final_type = "Mixed"
        elif has_analytic:
            final_type = "Analytic"
        elif has_holistic:
            final_type = "Holistic"
        
        data["proposed_name"] = _sanitize_str(data.get("proposed_name"))

        self.log(f"ğŸ§  Thought: {data.get('thought_process')}")
        self.log(f"ğŸ“ Strategy: {data.get('strategy')} | Type: {final_type}")
        if analytic_list:
            self.log(f"   [Analytic]: {analytic_list}")
        if holistic_list:
            self.log(f"   [Holistic]: {holistic_list}")
        if data.get("proposed_name"):
             self.log(f"ğŸ’¡ Proposing Name: {data.get('proposed_name')}")

        return data

    
    def handle_revoke(self, revoked_name):
        target_name = _sanitize_str(revoked_name)
        id_to_remove = None
        
        for tid, name in self.named_map.items():
            if name == target_name:
                id_to_remove = tid
                break
        
        if id_to_remove:
            del self.named_map[id_to_remove]
            if id_to_remove not in self.unnamed_candidates:
                self.unnamed_candidates.append(id_to_remove)
            self.log(f"ğŸ”„ Revoked Name: '{target_name}' (ID: {id_to_remove}). Returned to candidates.")
            return True
        else:
            self.log(f"âš ï¸ Revoke Failed: Name '{target_name}' not found in named_map.")
            return False

    def mark_current_target_done(self, agreed_name):
        if self.current_target_id:
            self.named_map[self.current_target_id] = agreed_name 
            if self.current_target_id in self.unnamed_candidates:
                self.unnamed_candidates.remove(self.current_target_id)
            self.log(f"âœ… Naming Completed: ID={self.current_target_id}, Name={agreed_name}")
            self.current_target_id = None

#ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼
class FollowerAgent:
    def __init__(self, name="Follower", max_char_count=50, csv_path="dataA.csv"):
        self.name = name
        self.max_char_count = max_char_count 
        self.data_manager = DataManager(csv_path)
        self.log_buffer: List[str] = []
        
        self.pn_probs = {label: 0.0 for label in self.data_manager.all_labels}
        self.pa_probs = {} 
        self.named_history = [] 
        self.named_map = {}
        
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.expression_counts = {"Holistic": 0, "Analytic": 0, "Mixed": 0}

    def log(self, msg):
        self.log_buffer.append(f"[{self.name}] {msg}")

    def respond(self, leader_utterance: str, full_history: str, proposed_name_by_leader: str = None) -> Dict:
        all_ids = self.data_manager.all_labels
        named_ids = self.named_history
        unnamed_ids = [lid for lid in all_ids if lid not in named_ids]

        unnamed_str = ", ".join(unnamed_ids)
        named_str = ", ".join(named_ids)

        if proposed_name_by_leader:
            task_instruction = f"""
ã€ã‚¿ã‚¹ã‚¯: å‘½åã®åˆæ„ã€‘
Leaderã‹ã‚‰åå‰ã€Œ{proposed_name_by_leader}ã€ãŒææ¡ˆã•ã‚Œã¾ã—ãŸã€‚
åˆæ„ã™ã‚‹å ´åˆ `accepted_name` ã«åå‰ã‚’å‡ºåŠ›ã€æ‹’å¦ã™ã‚‹å ´åˆ nullã€‚
åŸºæœ¬çš„ã«ææ¡ˆã•ã‚ŒãŸå‘½åã®æ‹’å¦ã¯è¡Œã‚ãªã„ã§ãã ã•ã„ï¼
ç™ºè©±å†…å®¹(utterance)ã«ã‚¿ãƒ³ã‚°ãƒ©ãƒ IDã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ï¼
ç™ºè©±å†…å®¹ã¯**æœ€å¤§ã§ã‚‚ã²ã‚‰ãŒãªã«ã—ãŸæ™‚ã«{self.max_char_count}æ–‡å­—ä»¥å†…**ã«åã‚ã¦ãã ã•ã„ï¼
ãŸã ã—ï¼Œç™ºè©±ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹éš›ã¯ï¼Œæ¼¢å­—ã‚„ã‚«ã‚¿ã‚«ãƒŠã«ã™ã¹ãèªå¥ã¯å¿…ãšå¤‰æ›ã—ã¦ã‹ã‚‰å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
éŸ³å£°å¯¾è©±ã¨ã—ã¦è‡ªç„¶ãªç™ºè©±ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
è‡ªç„¶å¯¾è©±ã‚’æƒ³å®šã™ã‚‹ãŸã‚ç®‡æ¡æ›¸ãã‚„æ‹¬å¼§æ›¸ããªã©ï¼Œè‡ªç„¶å¯¾è©±ã§ã¯ä½¿ç”¨ã•ã‚Œãªã„è¡¨ç¾ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼
**æŠ½å‡ºæŒ‡ç¤º:**
âš ï¸**é‡è¦:** æŠ½å‡ºå¯¾è±¡ã¯ã€**ã‚ãªãŸãŒä»Šå›ç”Ÿæˆã™ã‚‹ `utterance` ã®æ–‡è¨€ã®ã¿**ã§ã™ã€‚
ç›´å‰ã®Leaderã®ç™ºè¨€å†…å®¹ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
ç”Ÿæˆã—ãŸç™ºè©±(utterance)ã®ä¸­ã«å«ã¾ã‚Œã‚‹ä»¥ä¸‹ã®è¡¨ç¾ã‚’**æŠ½å‡ºã—ã¦ãƒªã‚¹ãƒˆå½¢å¼ã§**JSONã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
- **Analyticãªè¡¨ç¾**: éƒ¨åˆ†çš„ãƒ»å¹¾ä½•å­¦çš„ãªç‰¹å¾´
- **Holisticãªè¡¨ç¾**: å…¨ä½“çš„ãƒ»æŠ½è±¡çš„ãªå°è±¡

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSON)ã€‘
{{
  "pn_probabilities": {{...}}, 
  "pa_probabilities": {{...}}, 
  "accepted_name": "{proposed_name_by_leader}",
  "analytic_expressions": [],
  "holistic_expressions": [],
  "thought_process": "...",
  "utterance": "..."
}}
"""
        else:
            task_instruction = f"""
ã€ã‚¿ã‚¹ã‚¯: æ¨è«–ã€‘
ç›¸æ‰‹ã®ç™ºè©±å†…å®¹ã‹ã‚‰ã€ç›¸æ‰‹ãŒã©ã®ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®èª¬æ˜ã‚’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã‹æ¨æ¸¬ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸï¼Œç›¸æ‰‹ã®è¦‹ã¦ã„ã‚‹å›³å½¢ã¯è‡ªåˆ†ã®è¦‹ã¦ã„ã‚‹å›³å½¢ã¨å›è»¢è§’åº¦ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ï¼

**1. PN (æœªå‘½åã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®ç¢ºç‡) ã®è¨ˆç®—**
æœªå‘½åã®å€™è£œãƒªã‚¹ãƒˆ [{unnamed_str}] ã®ä¸­ã§ã€ç›¸æ‰‹ã®èª¬æ˜ãŒã©ã‚Œã«å½“ã¦ã¯ã¾ã‚‹ã‹ç¢ºç‡ã‚’æ¨å®šã—ã¦ãã ã•ã„ã€‚
- åˆè¨ˆãŒ 1.0 ã«ãªã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
- **é‡è¦:** ç›¸æ‰‹ã®èª¬æ˜ã«åˆè‡´ã™ã‚‹å€™è£œãŒ**ãªã„**ã¨åˆ¤æ–­ã—ãŸå ´åˆã¯ã€ç¢ºç‡ã‚’ä¸€å¾‹ä½ãè¨­å®šã—ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã«ã€Œåˆè‡´ãªã—ã€ã¨è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

**2. PA (å‘½åæ¸ˆã¿ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã¸ã®å½“ã¦ã¯ã¾ã‚Š) ã®è¨ˆç®—**
å‘½åæ¸ˆã¿ãƒªã‚¹ãƒˆ [{named_str}] ã®å„ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã«ã¤ã„ã¦ã€ã€Œç¾åœ¨ã®ç›¸æ‰‹ã®èª¬æ˜ãŒã©ã‚Œãã‚‰ã„å½“ã¦ã¯ã¾ã£ã¦ã—ã¾ã£ã¦ã„ã‚‹ã‹ã€ã‚’ç¢ºç‡(0.0~1.0)ã§æ¨å®šã—ã¦ãã ã•ã„ã€‚
- **ç›¸æ‰‹ã®èª¬æ˜ãŒã€ã‚ã‚‹å‘½åæ¸ˆã¿ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®ç‰¹å¾´ã¨é…·ä¼¼ã—ã¦ã„ã‚‹å ´åˆã€ãã®IDã®ç¢ºç‡ã‚’é«˜ãã—ã¦ãã ã•ã„**ã€‚
- å…¨ãå½“ã¦ã¯ã¾ã‚‰ãªã„å ´åˆã¯ 0.0 ã«è¿‘ã¥ã‘ã¦ãã ã•ã„ã€‚

ç™ºè©±å†…å®¹(utterance)ã«ã‚¿ãƒ³ã‚°ãƒ©ãƒ IDã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ï¼
ã¾ãŸç™ºè©±å†…å®¹ã¯ï¼Œç›´å‰ã«ç›¸æ‰‹ã®æç¤ºã—ã¦ããŸèª¬æ˜ã«å½“ã¦ã¯ã¾ã‚‹ã‚¿ãƒ³ã‚°ãƒ©ãƒ ãŒã‚ã£ãŸã‹ã©ã†ã‹ã¨ï¼Œå½“ã¦ã¯ã¾ã‚‹ã¨åˆ¤æ–­ã—ãŸæ ¹æ‹ ã®ç‰¹å¾´ã‚’å¿…ãšå«ã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ï¼
ç›¸æ‰‹ã®èª¬æ˜ãŒéƒ¨åˆ†çš„ãƒ»å¹¾ä½•å­¦çš„ãªç‰¹å¾´ã®ãƒ•ãƒ¬ãƒ¼ã‚ºãªã‚‰ã°å½“ã¦ã¯ã¾ã‚‹ã¨åˆ¤æ–­ã—ãŸæ ¹æ‹ ã®ç‰¹å¾´ã‚‚ã§ãã‚‹ãªã‚‰Analyticãªè¡¨ç¾ã‹ã‚‰ï¼Œ
ç›¸æ‰‹ã®èª¬æ˜ãŒå…¨ä½“çš„ãƒ»æŠ½è±¡çš„ãªå°è±¡ã®ãƒ•ãƒ¬ãƒ¼ã‚ºãªã‚‰ã°å½“ã¦ã¯ã¾ã‚‹ã¨åˆ¤æ–­ã—ãŸæ ¹æ‹ ã®ç‰¹å¾´ã‚‚ã§ãã‚‹ãªã‚‰Holisticãªè¡¨ç¾ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼
ç™ºè©±å†…å®¹ã¯**æœ€å¤§ã§ã‚‚ã²ã‚‰ãŒãªã«ã—ãŸæ™‚ã«{self.max_char_count}æ–‡å­—ä»¥å†…**ã«åã‚ã¦ãã ã•ã„ï¼
ãŸã ã—ï¼Œå½“ã¦ã¯ã¾ã‚‹ã¨åˆ¤æ–­ã—ãŸæ ¹æ‹ ã®ç‰¹å¾´ãŒå…¥ã‚‰ãªã„å ´åˆã¯ï¼Œ+5æ–‡å­—ç¨‹åº¦ãªã‚‰è¨±å®¹ã—ã¾ã™ï¼å½“ã¦ã¯ã¾ã‚‹ã¨åˆ¤æ–­ã—ãŸæ ¹æ‹ ã®ç‰¹å¾´ã‚’å«ã‚ã‚‹ã“ã¨ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ï¼
ãŸã ã—ï¼Œç™ºè©±ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹éš›ã¯ï¼Œæ¼¢å­—ã‚„ã‚«ã‚¿ã‚«ãƒŠã«ã™ã¹ãèªå¥ã¯å¿…ãšå¤‰æ›ã—ã¦ã‹ã‚‰å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
éŸ³å£°å¯¾è©±ã¨ã—ã¦è‡ªç„¶ãªç™ºè©±ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼
ãŸã ã—ï¼Œè‡ªç„¶å¯¾è©±ã‚’æƒ³å®šã™ã‚‹ãŸã‚ç®‡æ¡æ›¸ãã‚„æ‹¬å¼§æ›¸ããªã©ï¼Œè‡ªç„¶å¯¾è©±ã§ã¯ä½¿ç”¨ã•ã‚Œãªã„è¡¨ç¾ã¯ç™ºè©±å†…å®¹(utterance)ã§ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼
æ±ºã—ã¦æ–°ã—ã„ç‰¹å¾´ã‚„æ–°ã—ã„è¦ç´ ã®ææ¡ˆã¯è¡Œã‚ãªã„ã§ãã ã•ã„ï¼

**æ‹’å¦ã®æŒ‡ç¤º:**
ã‚‚ã—ç›´å‰ã®ç›¸æ‰‹ã®èª¬æ˜ã«åˆè‡´ã™ã‚‹ã‚¿ãƒ³ã‚°ãƒ©ãƒ ãŒå€™è£œã®ä¸­ã«**ä¸€ã¤ã‚‚ç„¡ã„**ã¨åˆ¤æ–­ã—ãŸå ´åˆã¯ã€`utterance` ã«ã¯ **ã€Œã‚ã‚Šã¾ã›ã‚“ã€** ã¨ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªè¨€è‘‰ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

**æŠ½å‡ºæŒ‡ç¤º:**
âš ï¸**é‡è¦:** æŠ½å‡ºå¯¾è±¡ã¯ã€**ã‚ãªãŸãŒä»Šå›ç”Ÿæˆã™ã‚‹ `utterance` ã®æ–‡è¨€ã®ã¿**ã§ã™ã€‚
ç›´å‰ã®Leaderã®ç™ºè¨€å†…å®¹ï¼ˆ"{leader_utterance}"ï¼‰ã‹ã‚‰æŠ½å‡ºã—ãªã„ã§ãã ã•ã„ã€‚
ç”Ÿæˆã—ãŸç™ºè©±(utterance)ã®ä¸­ã«å«ã¾ã‚Œã‚‹ä»¥ä¸‹ã®è¡¨ç¾ã‚’**æŠ½å‡ºã—ã¦ãƒªã‚¹ãƒˆå½¢å¼ã§**JSONã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚è©²å½“ãªã—ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆ `[]` ã«ã—ã¦ãã ã•ã„ã€‚
- **Analyticãªè¡¨ç¾**: éƒ¨åˆ†çš„ãƒ»å¹¾ä½•å­¦çš„ãªç‰¹å¾´ã®ãƒ•ãƒ¬ãƒ¼ã‚º
- **Holisticãªè¡¨ç¾**: å…¨ä½“çš„ãƒ»æŠ½è±¡çš„ãªå°è±¡ã®ãƒ•ãƒ¬ãƒ¼ã‚º
ã¾ãŸï¼Œthought_processã«ã¯ãªãœãã®ã‚ˆã†ã«ç¢ºç‡ã‚’å¤‰å‹•ã•ã›ãŸã®ã‹ä¸å¯§ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (JSON)ã€‘
{{
  "pn_probabilities": {{"A2": 0.7, "B1": 0.3, ...}}, 
  "pa_probabilities": {{"A1": 0.8, ...}}, // â˜…é‡è¦: èª¬æ˜ã«åˆè‡´ã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ç¢ºç‡(é«˜ã„=æ’¤å›å€™è£œ)
  "accepted_name": null,
  "analytic_expressions": [],
  "holistic_expressions": [],
  "thought_process": "...",
  "utterance": "..."
}}
"""

        prompt = f"""
ã‚ãªãŸã¯ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã‚²ãƒ¼ãƒ ã®ã€Œå›ç­”è€…ï¼ˆFollowerï¼‰ã€ã§ã™ã€‚

ã€å…¨ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®ç‰¹å¾´ãƒªã‚¹ãƒˆã€‘
{self.data_manager.context_text}

ã€ç¾åœ¨ã®çŠ¶æ³ã€‘
- æœªå‘½åå€™è£œ: [{unnamed_str}]
- å‘½åæ¸ˆã¿ï¼ˆé™¤å¤–å¯¾è±¡ï¼‰: [{named_str}]

ã€ã“ã“ã¾ã§ã®å…¨å¯¾è©±å±¥æ­´ã€‘
{full_history}

ã€å‡ºé¡Œè€…ã®æœ€æ–°ã®ç™ºè¨€ã€‘
"{leader_utterance}"

{task_instruction}
"""
        res = call_api_with_retry(lambda: client.chat.completions.create(
            model="gpt-5", 
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            reasoning_effort="low"
        ))
        
        self.total_prompt_tokens += res.usage.prompt_tokens
        self.total_completion_tokens += res.usage.completion_tokens
        
        try:
            data = json.loads(res.choices[0].message.content)
        except:
            data = {
                "pn_probabilities": {}, "pa_probabilities": {}, 
                "accepted_name": None, "utterance": "..."
            }

        analytic_list = data.get("analytic_expressions", [])
        holistic_list = data.get("holistic_expressions", [])
        
        if not isinstance(analytic_list, list): analytic_list = []
        if not isinstance(holistic_list, list): holistic_list = []

        has_analytic = len(analytic_list) > 0
        has_holistic = len(holistic_list) > 0
        final_type = "None"

        self.expression_counts["Analytic"] += len(analytic_list)
        self.expression_counts["Holistic"] += len(holistic_list)

        if has_analytic and has_holistic:
            final_type = "Mixed"
        elif has_analytic:
            final_type = "Analytic"
        elif has_holistic:
            final_type = "Holistic"

        self.pn_probs = data.get("pn_probabilities", {})
        total_pn = sum(float(v) for v in self.pn_probs.values())
        if total_pn > 0:
            self.pn_probs = {k: float(v)/total_pn for k, v in self.pn_probs.items()}
        
        raw_pa = data.get("pa_probabilities", {})
        self.pa_probs = {}
        for named_id in self.named_history:
            if named_id in raw_pa:
                self.pa_probs[named_id] = float(raw_pa[named_id])
            else:
                self.pa_probs[named_id] = 0.0
        
        data["revoke_request"] = None 
        data["accepted_name"] = _sanitize_str(data.get("accepted_name"))
        
        self.log(f"ğŸ¤” Thought: {data.get('thought_process')}")
        self.log(f"ğŸ“ Strategy: Type:{final_type}")
        if analytic_list:
            self.log(f"   [Analytic]: {analytic_list}")
        if holistic_list:
            self.log(f"   [Holistic]: {holistic_list}")
        
        sorted_pn = sorted(self.pn_probs.items(), key=lambda x: float(x[1]), reverse=True)
        pn_str = ", ".join([f"{k}:{v:.2f}" for k, v in sorted_pn])
        self.log(f"ğŸ“Š PN (Unnamed): {{{pn_str}}}")
        
        if self.pa_probs:
            sorted_pa = sorted(self.pa_probs.items(), key=lambda x: float(x[1]), reverse=True)
            pa_str = ", ".join([f"{k}:{v:.2f}" for k, v in sorted_pa])
            self.log(f"ğŸ›¡ï¸ PA (Named - Matching?): {{{pa_str}}}")

        if data.get("accepted_name"):
            self.log(f"ğŸ¤ Accepted Name: {data.get('accepted_name')}")

        return data

    def update_named_status(self, target_id, name):
        if target_id not in self.named_history:
            self.named_history.append(target_id)
            self.named_map[target_id] = name 
            self.pa_probs[target_id] = 0.0

    def handle_revoke_accepted(self, revoked_id):
        if revoked_id in self.named_history:
            self.named_history.remove(revoked_id)
            if revoked_id in self.named_map:
                del self.named_map[revoked_id] 
            if revoked_id in self.pa_probs:
                del self.pa_probs[revoked_id]

# ==========================================
# 4. ã‚²ãƒ¼ãƒ ãƒã‚¹ã‚¿ãƒ¼ (é€²è¡Œç®¡ç†)
# ==========================================
class GameMaster:
    def __init__(self, session_id, max_turns=30, max_char_count=50, leader_data="data.csv", follower_data="dataA.csv"):
        self.session_id = session_id
        self.max_turns = max_turns
        self.max_char_count = max_char_count 
        
        self.leader = LeaderAgent(name="Leader", max_char_count=max_char_count, csv_path=leader_data)
        self.follower = FollowerAgent(name="Follower", max_char_count=max_char_count, csv_path=follower_data)
        self.data_manager = self.leader.data_manager 
        
        self.chronological_log = []
        self.conversation_log = [] 
        self.turn_count = 0
        self.is_naming_phase = False
        self.no_match_counter = 0

    def log_system(self, msg):
        self.chronological_log.append(f"[System] {msg}")

    def _capture_logs(self):
        if self.leader.log_buffer:
            self.chronological_log.extend(self.leader.log_buffer)
            self.leader.log_buffer = []
        if self.follower.log_buffer:
            self.chronological_log.extend(self.follower.log_buffer)
            self.follower.log_buffer = []

    def _get_cost_summary(self):
        l_in = self.leader.total_prompt_tokens
        l_out = self.leader.total_completion_tokens
        f_in = self.follower.total_prompt_tokens
        f_out = self.follower.total_completion_tokens
        total = l_in + l_out + f_in + f_out
        l_counts = self.leader.expression_counts
        f_counts = self.follower.expression_counts 
        
        summary = (
            f"\nğŸ“Š Expression Counts:\n"
            f"   [Leader]\n"
            f"     - Holistic: {l_counts['Holistic']}\n"
            f"     - Analytic: {l_counts['Analytic']}\n"
            f"     - Mixed   : {l_counts['Mixed']}\n"
            f"   [Follower]\n"
            f"     - Holistic: {f_counts['Holistic']}\n"
            f"     - Analytic: {f_counts['Analytic']}\n"
            f"     - Mixed   : {f_counts['Mixed']}\n\n"
            f"ğŸ’° Token Usage Summary (Session {self.session_id}):\n"
            f"   [Leader]   In: {l_in}, Out: {l_out} (Total: {l_in + l_out})\n"
            f"   [Follower] In: {f_in}, Out: {f_out} (Total: {f_in + f_out})\n"
            f"   --------------------------------------------------\n"
            f"   [TOTAL]    {total} tokens\n"
        )
        return summary

    def save_logs(self):
        file_base = f"s{self.session_id}_Char{self.max_char_count}"
        cost_summary = self._get_cost_summary()
        
        txt_path = os.path.join(SAVE_DIR, f"log_{file_base}.txt")
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(f"=== Session {self.session_id} (MaxChar={self.max_char_count}) ===\n")
            f.write(f"Date: {datetime.now().isoformat()}\n\n")
            for line in self.chronological_log:
                f.write(line + "\n")
            f.write("\n" + "="*40 + "\n")
            f.write(cost_summary)
            f.write("\n" + "="*40 + "\n")
            f.write("ğŸ“Š Final Naming Results:\n")
            f.write(f"[Leader]   {json.dumps(self.leader.named_map, ensure_ascii=False)}\n")
            f.write(f"[Follower] {json.dumps(self.follower.named_map, ensure_ascii=False)}\n")
                
        json_path = os.path.join(SAVE_DIR, f"data_{file_base}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(self.conversation_log, f, indent=2, ensure_ascii=False)
        
        dialogue_path = os.path.join(SAVE_DIR, f"dialogue_{file_base}.txt")
        with open(dialogue_path, "w", encoding="utf-8") as f:
            for entry in self.conversation_log:
                speaker = entry.get("speaker")
                text = entry.get("text")
                if speaker and text and text != "...":
                    f.write(f"{speaker}: {text}\n")
            
        return txt_path

    def check_similarity(self, text1, text2, threshold=0.5):
        if not text1 or not text2: return False
        v1 = self.data_manager.encode_text(text1)
        v2 = self.data_manager.encode_text(text2)
        sim = cosine_similarity([v1], [v2])[0][0]
        self.log_system(f"ğŸ“ Similarity: {sim:.3f}")
        return sim >= threshold

    def run_simulation(self):
        self.log_system(f"ğŸš€ Started Session {self.session_id}. Max Char = {self.max_char_count}")
        
        full_history_text = ""
        self.is_naming_phase = False 
        
        consecutive_no_match = 0
        last_valid_follower_probs = {} 

        while self.turn_count < self.max_turns:
            self.turn_count += 1
            self.log_system(f"--- Turn {self.turn_count} ---")

            if not self.leader.current_target_id:
                prev_target = self.leader.current_target_id
                tid = self.leader.select_next_target()
                if tid != prev_target:
                    consecutive_no_match = 0 
                if not tid:
                    self.log_system("ğŸ‰ All Tangrams Identified!")
                    break
                self.is_naming_phase = False 
            
            # 1. ãƒªãƒ¼ãƒ€ç™ºè©±
            leader_data = self.leader.generate_utterance(
                full_history_text, 
                is_naming_phase=self.is_naming_phase
            )
            leader_utt = leader_data.get("utterance", "...")
            proposed_name = leader_data.get("proposed_name")
            
            self._capture_logs()
            self.conversation_log.append({
                "turn": self.turn_count, "speaker": "Leader", 
                "text": leader_utt,
                "analytic_expressions": leader_data.get("analytic_expressions", []),
                "holistic_expressions": leader_data.get("holistic_expressions", []),
                "target_id": self.leader.current_target_id
            })
            self.log_system(f"ğŸ—£ï¸ Leader: {leader_utt}")
            full_history_text += f"Leader: {leader_utt}\n"

            # 2. ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¿œç­”
            follower_data = self.follower.respond(leader_utt, full_history_text, proposed_name_by_leader=proposed_name)
            follower_utt = follower_data.get("utterance", "...")
            
            current_probs = follower_data.get("pn_probabilities", {})
            valid_ids = self.leader.data_manager.all_labels 
            valid_probs = {}
            for k, v in current_probs.items():
                if k in valid_ids:
                    try: valid_probs[k] = float(v)
                    except: pass
            
            if valid_probs:
                last_valid_follower_probs = valid_probs
            
            max_pn = max(valid_probs.values()) if valid_probs else 0.0
            
            # åˆè‡´ãªã—åˆ¤å®š
            is_rejected = False
            if "ã‚ã‚Šã¾ã›ã‚“" in follower_utt:
                is_rejected = True
                self.log_system(f"âš ï¸ Follower says 'Arimasen'.")
            elif max_pn < 0.2 and valid_probs: 
                is_rejected = True
                self.log_system(f"âš ï¸ Low probability detected (Max PN: {max_pn:.2f}).")

            if is_rejected and not self.is_naming_phase:
                consecutive_no_match += 1
                self.log_system(f"âš ï¸ No match count: {consecutive_no_match}/2")
            else:
                consecutive_no_match = 0

            # æ’¤å›å®Ÿè¡Œåˆ¤å®šï¼ˆ2å›é€£ç¶šä¸ä¸€è‡´ & å‘½åå±¥æ­´ã‚ã‚Šï¼‰
            revoke_req = None
            revoke_req_name = None 
            
            if consecutive_no_match >= 2 and self.follower.named_history:
                # 1. æ’¤å›å¯¾è±¡ã®IDã‚’æ±ºå®šã™ã‚‹
                pa_probs = follower_data.get("pa_probabilities", {})
                valid_pa = {k: float(v) for k, v in pa_probs.items() if k in self.follower.named_history}
                
                revoke_target_id = None
                
                if valid_pa:
                    # PAæœ€å¤§ã®ã‚‚ã®ã‚’æ’¤å›å¯¾è±¡ã«
                    revoke_target_id = max(valid_pa, key=valid_pa.get)
                else:
                    revoke_target_id = self.follower.named_history[-1]
                
                # 2. IDã‹ã‚‰åå‰ã‚’å–å¾—
                revoke_name = self.follower.named_map.get(revoke_target_id)
                
                # 3. å¯¾è©±ç”Ÿæˆã¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š (åå‰ãŒç‰¹å®šã§ããŸå ´åˆã®ã¿)
                if revoke_name:
                    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã®ç™ºè¨€ (å¼·åˆ¶æŒ¿å…¥)
                    f_msg = f"ã‚ãªãŸã®èª¬æ˜ã ã¨å‘½åæ¸ˆã¿ã®ã€{revoke_name}ã€ãŒå½“ã¦ã¯ã¾ã‚Šã¾ã™ã€‚é–“é•ãˆã¦ã„ãŸã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã‚„ã‚Šç›´ã—ã¾ã›ã‚“ã‹ï¼Ÿ"
                    self.log_system(f"ğŸ—£ï¸ Follower (Auto-Revoke): {f_msg}")
                    self.conversation_log.append({"turn": self.turn_count, "speaker": "Follower", "text": f_msg})
                    full_history_text += f"Follower: {f_msg}\n"
                    
                    # ãƒªãƒ¼ãƒ€ã®ç™ºè¨€ (å¼·åˆ¶æŒ¿å…¥)
                    l_msg = f"ã‚ã‹ã‚Šã¾ã—ãŸã€‚ã€{revoke_name}ã€ã¨å‘½åã—ãŸã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®å‘½åã‚’æ’¤å›ã—ã¾ã™ã€‚"
                    self.log_system(f"ğŸ—£ï¸ Leader (Auto-Revoke): {l_msg}")
                    self.conversation_log.append({"turn": self.turn_count, "speaker": "Leader", "text": l_msg})
                    full_history_text += f"Leader: {l_msg}\n"
                    
                    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã®ç™ºè¨€ (å¼·åˆ¶æŒ¿å…¥)
                    f_msg_2 = "ã‚ã‹ã‚Šã¾ã—ãŸ"
                    self.log_system(f"ğŸ—£ï¸ Follower (Auto-Revoke): {f_msg_2}")
                    self.conversation_log.append({"turn": self.turn_count, "speaker": "Follower", "text": f_msg_2})
                    full_history_text += f"Follower: {f_msg_2}\n"
                    
                    revoke_req = revoke_target_id
                    revoke_req_name = revoke_name
                    
                    consecutive_no_match = 0 # ã‚«ã‚¦ãƒ³ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ

            accepted_name = follower_data.get("accepted_name")
            
            self._capture_logs()
            if not revoke_req:
                self.conversation_log.append({
                    "turn": self.turn_count, "speaker": "Follower", 
                    "text": follower_utt,
                    "analytic_expressions": follower_data.get("analytic_expressions", []),
                    "holistic_expressions": follower_data.get("holistic_expressions", []),
                    "accepted_name": accepted_name,
                    "revoke_request": revoke_req
                })
                self.log_system(f"ğŸ—£ï¸ Follower: {follower_utt}")
                full_history_text += f"Follower: {follower_utt}\n"

            # æ’¤å›å‡¦ç†
            if revoke_req:
                success = self.leader.handle_revoke(revoke_req_name)
                if success:
                    f_id_to_remove = None
                    for tid, name in self.follower.named_map.items():
                        if name == revoke_req_name:
                            f_id_to_remove = tid
                            break
                    
                    if f_id_to_remove:
                        self.follower.handle_revoke_accepted(f_id_to_remove)
                    
                    self.log_system(f"ğŸ”„ Revoke Accepted for '{revoke_req_name}'. Resetting target.")
                    self.leader.current_target_id = None 
                    self.is_naming_phase = False
                    continue

            # ----------------------------------------------------
            # 5. å‘½ååˆæ„ (ã™ã‚Œé•ã„è¨±å®¹)
            # ----------------------------------------------------
            if proposed_name and accepted_name:
                self.log_system(f"âœ… Naming Agreement Reached: {accepted_name}")
                
                leader_target_id = self.leader.current_target_id
                self.leader.mark_current_target_done(accepted_name)
                
                if last_valid_follower_probs:
                    follower_believed_id = max(last_valid_follower_probs, key=last_valid_follower_probs.get)
                else:
                    self.log_system(f"âš ï¸ No valid probability history. Fallback to Leader ID.")
                    follower_believed_id = leader_target_id
                
                self.follower.update_named_status(follower_believed_id, accepted_name)
                
                if leader_target_id != follower_believed_id:
                    self.log_system(f"âš ï¸ Misunderstanding! Leader named {leader_target_id}, but Follower named {follower_believed_id}.")
                
                self.leader.current_target_id = None
                self.is_naming_phase = False 
                continue

            if not self.is_naming_phase:
                if is_rejected:
                    self.log_system(f"â³ Follower rejected. Skipping similarity check.")
                else:
                    is_similar = self.check_similarity(leader_utt, follower_utt, threshold=0.5)
                    if is_similar:
                        self.log_system(f"âœ¨ High Similarity Detected. Switching to NAMING PHASE.")
                        self.is_naming_phase = True
            
            elif not self.is_naming_phase:
                 self.log_system(f"â³ Continue Explanation.")

        self.log_system("ğŸ Simulation Finished.")
        print(self._get_cost_summary())
        path = self.save_logs()
        return {
            "session_id": self.session_id,
            "log_file": path,
            "turns": self.turn_count,
            "status": "success"
        }

# ==========================================
# 5. ä¸¦åˆ—å®Ÿè¡Œãƒ©ãƒƒãƒ‘ãƒ¼
# ==========================================
def run_single_session_wrapper(args):
    session_id, max_turns, silent, max_char, l_path, f_path = args
    with suppress_output(suppress=silent):
        try:
            gm = GameMaster(session_id, max_turns, max_char_count=max_char, 
                            leader_data=l_path, follower_data=f_path)
            return gm.run_simulation()
        except Exception as e:
            return {"session_id": session_id, "status": "error", "error": str(e)}

def run_mixed_experiments(config_list: List[Dict], num_experiments_per_config=3, max_workers=3, max_turns=30):
    print(f"âš¡ Starting Leader-Follower Experiments (Parallel={max_workers})...")
    
    DataManager("dataR.csv") 

    all_tasks = []
    global_session_id = 0

    for config in config_list:
        max_char = config.get("max_char_count", 50)
        l_path = config.get("leader_data", "data.csv")
        f_path = config.get("follower_data", "dataA.csv")
        
        for _ in range(num_experiments_per_config):
            task_args = (global_session_id, max_turns, True, max_char, l_path, f_path)
            all_tasks.append(task_args)
            global_session_id += 1

    total_tasks = len(all_tasks)
    print(f"ğŸ“‹ Total Tasks: {total_tasks}")
    
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(run_single_session_wrapper, t) for t in all_tasks]
        
        for future in tqdm(concurrent.futures.as_completed(futures), total=total_tasks, desc="Progress"):
            res = future.result()
            results.append(res)
            
            if res["status"] == "success":
                tqdm.write(f"  âœ… ID:{res['session_id']} | Turns:{res['turns']}")
            else:
                tqdm.write(f"  âŒ ID:{res['session_id']} | Error: {res.get('error')}")

    print("\nğŸ“Š Done.")
    return results

if __name__ == "__main__":
    experiment_configs = [
        {
            "max_char_count": 30, #1ç™ºè©±ã‚ãŸã‚Šã®æ–‡å­—æ•°
            "leader_data": "dataLAB.csv",#ãƒªãƒ¼ãƒ€ã«æ¸¡ã™ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®ç‰¹å¾´ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            "follower_data": "dataFAB.csv" #ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã«æ¸¡ã™ã‚¿ãƒ³ã‚°ãƒ©ãƒ ã®ç‰¹å¾´ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        },
    ]

    final_results = run_mixed_experiments(
        config_list=experiment_configs,
        num_experiments_per_config=2,#æ¡ä»¶ã”ã¨ã®å®Ÿè¡Œå›æ•°
        max_workers=10,#æœ€å¤§ä¸¦åˆ—å®Ÿè¡Œæ•°                
        max_turns=50#æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°
    )